# Model-Evaluation        ![](https://img.shields.io/badge/Haribaskar-Dhanabalan-brightgreen.svg?colorB=#ADFF2F)

This Package is developed to evaluate the classification | Regression models based on different metrics.

The use case I have solved in this module is to get the actual and predicted value from the users (pred value should be probablity for
classification and continious value for regression). Based on this input it creates a Evaluation table which contains these columns:

**Regression-** `Unique_ModelID`,`Model_Reference_name`,`mean_absolute_error`, `mean_squared_error`, `mean_squared_log_error`,`median_absolute_error`, `r2_score`, `actual_pred_details`,`Time_stamp`.

**Classification-** `Unique_ModelID`,`Model_Reference_name`,`Threshold`,`TP`,`FP`,`FN`,`TN`,`Accuracy`,
`Precision`,`recall`,`f1`,`mcc`,`roc_auc`,`actual_pred_details`,`Time_stamp`.

From this table we can get threshold value which maximises a particular metrics (the metrics can be choosen according the business requirement - for classification we can go with f1 score for most of the cases (or) if you are concentrating more on the negative 
class then we can select recall etc.,)

---
#### Code Structure:
```python
class Evaluation():
  .........
  return(metrics_table)
class evaluation_plots():
  .........
  display plots
class ModelEvaluation(Evaluation,evaluation_plots):
  .........
```

**Evaluation():**
*This Module is used to generate the metrics table for different sets of thresold values for the classification, and
outputs the set of metrics for the regression problem. (which will be later used to plot the graphs and to understand
the model behavior and imporove the model performance)
This Evaluation Class will deal with the classification & Regression models.
This returns the metrics for different set of thresholds*

**evaluation_plots():**
*This class is used to generate the graphs based on the metrics table, which was generated by the above evaluation 
module. These plots can be exported to the local storage for our future reference.*

**ModelEvaluation():**
*This is the main evaluator module - which runs the above two classes and saves the results based on the user request.*

---
## Sample Test Script:

```python
import evaluation_module as eval_mod
import random

## Sample data: (Actual_value/ Predicted_probablity values)
random.seed(9005)
pred_prob = [random.uniform(0,1) for i in range(100) ]
actual = [random.sample(range(0,2),1)[0] for i in range(100)]

# Initializing the module:
evalu = eval_mod.ModelEvaluation(actual = actual, pred = pred_prob)

# Model evaluation call:
metrics_db = evalu.evaluate(evaluate_save=True,plots_show=False)

# Model Comparison call:
evalu.Compare_models(evaluate_db = metrics_db, model_id = [metrics_db['Unique_ModelID'][0]],comparison_metrics = ['Accuracy','mcc'])
```
---

## Decile level Analysis:

The Decile level analysis plot is done based on `Cumulative population`, `Accuracy in Decile` and `True Covered` - (which is a replication from `Rapid Miner`) from this plot also we can come up with the best threshold and also we can understand our population ie., how the population is distributed from the positive to negative scale (positive is Target =1, negative is Target = 0 - which can change based on the business scenerio).

![Alt text](https://us.v-cdn.net/6030995/uploads/lithium_attachments/image/serverpage/image-id/3131iAC6D608E14231F98/question.png?raw=true "Sample decile plot from rapid miner")

**How to Interpret the above graph?** *The model is doing a good job in detecting the class 'Snowy Weather'.  If you look into the predictions for all our unseen test cases and sort them according to the confidence for 'Snowy Weather', most of the snow days have indeed been covered when the model was more sure that it will snow.  For example, 71% of all snow days have been covered by the top 30% cases where the model was most sure it will snow."
The last sentence above also makes clear why lift / gain charts are in particular useful for marketing campaigns.  Here it would read "we only send our campaign to the top 30% of our leads and got 71% of all possible responses.  This saved us 70% of costs for almost the same outcome!*

---
## Contributing

Patches are welcome, preferably as pull requests.

---
### Other Reference for evaluation:
[Documentation here](http://edublancas.github.io/sklearn-evaluation) - sklearn evaluation 

